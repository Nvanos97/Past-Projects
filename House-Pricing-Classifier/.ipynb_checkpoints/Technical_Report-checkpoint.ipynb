{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nathan Vanos and Scott Kopczynski\n",
    "# Project Title: The \"Sales\" Vector Machine\n",
    "# CPSC 310, Professor Sprint, Final Project\n",
    "# Date: 05/09/2019\n",
    "\n",
    "## Introduction\n",
    "************\n",
    "The classification project that we partook in was certainly one of the most rewarding tasks of the year. That being said, it was quite difficult. This may have been, at least in part, because of the dataset we chose to classify. Our table is essentially a collection of houses that contains listings of various real estate attributes. So, what was the task we chose? We decided that it would be a good idea to attempt to predict house prices! We discovered a great many things along the way, including how bad of an idea it is to run Naïve Bayes over all the attributes in the dataset. We also found out that ensemble classifiers, interestingly enough, have the highest accuracy, and support vector machines (at least with this data) have a far less impressive accuracy. \n",
    "************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Results\n",
    "************\n",
    "\n",
    "************"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "************\n",
    "Ultimately, our project consisted of cleaning the dataset, discretizing continuous attributes, creating a plot with SKLearn’s support vector machine, fitting the data to that plot for predictions, and finally, using stratified k-fold cross validation to compute the accuracy. The dataset proved to be challenging for us in several ways: the number of attributes, the presence of NA values, and lastly, the need to normalize the sale price. There was ultimately more work we had to do before implementing the classifier than there was for implementing the classifier itself. Our classifier’s performance wound up being less than satisfactory, and we definitely expected its accuracy to be higher. The low accuracy could either be a result of this particular dataset, or it could also be due to our lack of experience using support vector machines. We were unsure exactly how to choose the best C and gamma values for the SVC function, and we believe that if we had more time, and we figured out how to optimize these values, the support vector machine’s accuracy could improve drastically. But at the end of the day, we still learned a lot, and now we have a basic understanding of a new classifier to take with us into the real world.\n",
    "************"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
