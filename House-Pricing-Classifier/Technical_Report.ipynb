{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nathan Vanos and Scott Kopczynski\n",
    "# Project Title: The \"Sales\" Vector Machine\n",
    "# CPSC 310, Professor Sprint, Final Project\n",
    "# Date: 05/09/2019\n",
    "\n",
    "## Introduction\n",
    "************\n",
    "The classification project that we partook in was certainly one of the most rewarding tasks of the year. That being said, it was quite difficult. This may have been, at least in part, because of the dataset we chose to classify. Our table is essentially a collection of houses that contains listings of various real estate attributes. So, what was the task we chose? We decided that it would be a good idea to attempt to predict house prices! We discovered a great many things along the way, including how bad of an idea it is to run Naïve Bayes over all the attributes in the dataset. We also found out that ensemble classifiers, interestingly enough, have the highest accuracy, and support vector machines (at least with this data) have a far less impressive accuracy. \n",
    "************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Results\n",
    "************\n",
    "So, what is the amazing classifier that we elected to use on this dataset? We chose the one and only support vector machine from SKLearn. The general approach we used to classify the data with our support vector machine was similar to the strategy we employed in our other assignments throughout the semester. \n",
    "************\n",
    "### Cleaning the Data\n",
    "************\n",
    "First, our classifier cleans the data so that there are no “NA” values. We ultimately elected to do this in several steps, which are listed below. The program eventually replaces the NAs with the mode of the various attributes. \n",
    "```\n",
    "# ** clean data\n",
    "    training_table = clean_alley(training_table, header)\n",
    "    training_table  = clean_garage(training_table, header)\n",
    "    training_table  = clean_basement(training_table, header)\n",
    "    training_table  = fill_with_best_val(training_table, header)\n",
    "    training_table  = replace_with_mode(training_table, header)\n",
    "    training_table  = remove_nas(training_table)\n",
    "```\n",
    "************\n",
    "\n",
    "### Discretizing the Data\n",
    "************\n",
    "Next, the classifier discretizes the table. This is also a multi-step process, and we eventually had to write an ugly boolean function to implement it. The function is_continuous() determines if an attribute is continuous, and if this results to true, the attribute is converted to a categorical one. Since the class label itself is a continuous attribute, we had to include sale_prices = np.log1p(sale_prices) in order to normalize the prices so they could be more effectively fit. Lastly, we chose to split the continuous attributes into ten bins, mainly because we wanted a number that wasn’t too large or too small.  \n",
    "```\n",
    "# discretizes continuous attributes\n",
    "def discretize_table(header, table):\n",
    "    sale_prices = get_column_float(table, header.index(\"SalePrice\"))\n",
    "    sale_prices = np.log1p(sale_prices)\n",
    "    for row in table:\n",
    "        row[header.index(\"SalePrice\")] = sale_prices[table.index(row)]\n",
    "    # convert all continuous attributes to categorical ones\n",
    "    for att in range(0, len(header)):\n",
    "        if is_continuous(header[att]):\n",
    "            values = get_column_float(table, att)\n",
    "            cutoffs = compute_equal_widths_cutoffs(values, 10)\n",
    "            convert_to_categorical(cutoffs, att, table)\n",
    "            \n",
    "# checks if an attribute is continuous\n",
    "def is_continuous(column):\n",
    "    if column == \"LotArea\":\n",
    "        return True\n",
    "    if column == \"BsmtFinSF1\":\n",
    "        return True\n",
    "    if column == \"BsmtFinSF2\":\n",
    "        return True\n",
    "    if column == \"BsmtUnfSF\":\n",
    "        return True\n",
    "    if column == \"TotalBsmtSF\":\n",
    "        return True\n",
    "    if column == \"1stFlrSF\":\n",
    "        return True\n",
    "    if column == \"2ndFlrSF\":\n",
    "        return True\n",
    "    if column == \"LowQualFinSF\":\n",
    "        return True\n",
    "    if column == \"GrLivArea\":\n",
    "        return True\n",
    "    if column == \"GarageArea\":\n",
    "        return True\n",
    "    if column == \"WoodDeckSF\":\n",
    "        return True\n",
    "    if column == \"OpenPorchSF\":\n",
    "        return True\n",
    "    if column == \"EnclosedPorch\":\n",
    "        return True\n",
    "    if column == \"3SsnPorch\":\n",
    "        return True\n",
    "    if column == \"ScreenPorch\":\n",
    "        return True\n",
    "    if column == \"PoolArea\":\n",
    "        return True\n",
    "    if column == \"MiscVal\":\n",
    "        return True\n",
    "    if column == \"SalePrice\":\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "```\n",
    "************\n",
    "\n",
    "### Plotting the clf\n",
    "************\n",
    "Since implementing all of the features of a support vector machine would be ridiculously complicated, we ultimately decided to just use SKLearn’s library to implement one. Even so, we still feel that we learned a lot about this classification technique. There are two key steps. The first step is fitting the data, and in order to do this, the program creates an x and y axis, somewhat like a graph. The y-axis contains the class, and the x-axis contains the rest of the attributes. Then, the data is fit with a call to svm’s SVC function. An important parameter for SVC is the C parameter, which penalizes slack variables that can lead to miscalculation. A higher value of C leads to more variables being penalized, so a well-chosen C can make or break this classifier’s performance. \n",
    "```\n",
    "y_vals = get_column(train_data, len(table[0])-1)\n",
    "x_vals = get_attribs(train_data)\n",
    "clf = svm.SVC(C=10000000000000, cache_size=200, class_weight=None, coef0=0.0,\n",
    "                      decision_function_shape='ovr', degree=80, gamma=0.00000000000001, kernel='rbf',\n",
    "                      max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "                      tol=0.001, verbose=False)\n",
    "```\n",
    "************\n",
    "### Fitting the Data\n",
    "************\n",
    "The second step that the program takes to classify the data is to fit the data using the clf, which it basically does by generating a best fit line through the new data points.  \n",
    "```\n",
    "clf.fit(x_vals, y_vals)\n",
    "result = clf.predict([row[:-2]]) # code here is in a for loop\n",
    "predicted.append(result)\n",
    "```\n",
    "************\n",
    "\n",
    "### Stratified K-Fold Cross Validation\n",
    "************\n",
    "To test our classifier’s predictive ability, our program uses the classic approach of stratified k-fold cross validation, in which k folds of the original data are generated, and then each one is used in turn as the test set for a total of k “runs” of the classifier. The accuracy of each run is recorded, then at the end, the average accuracy is calculated. The average accuracy is the metric we used to rate our classifier’s performance.\n",
    "```\n",
    "# classify the dataset with support vector machine\n",
    "def classify_with_svm(table):\n",
    "    accuracies = []\n",
    "    table = encode_data(table)\n",
    "    k_folds = knn_naive_classifier.determine_stratified_k_fold(table, 10)\n",
    "    correct_counts = []\n",
    "    predicted = []\n",
    "    actual = []\n",
    "    for i in range(0, len(k_folds)):\n",
    "        train_data = []\n",
    "        for j in range(0, len(k_folds)):\n",
    "            if j != i:\n",
    "                train_data+= k_folds[j]\n",
    "        y_vals = get_column(train_data, len(table[0])-1)\n",
    "        x_vals = get_attribs(train_data)\n",
    "        #x_vals = encode_data(x_vals)\n",
    "        clf = svm.SVC(C=10000000000000, cache_size=200, class_weight=None, coef0=0.0,\n",
    "                      decision_function_shape='ovr', degree=80, gamma=0.00000000000001, kernel='rbf',\n",
    "                      max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "                      tol=0.001, verbose=False)\n",
    "        clf.fit(x_vals, y_vals)\n",
    "        correct_count = 0\n",
    "        for row in k_folds[i]:\n",
    "            result = clf.predict([row[:-2]])\n",
    "            predicted.append(result)\n",
    "            actual.append(row[len(row)-1])\n",
    "            if(result == row[len(row)-1]):\n",
    "                correct_count += 1\n",
    "        correct_counts.append(correct_count)\n",
    "    accuracy = (sum(correct_counts) / len(correct_counts))/len(k_folds[0])\n",
    "    accuracies.append(accuracy)\n",
    "    return accuracies[0]\n",
    "```\n",
    "************\n",
    "\n",
    "### Other Classifiers\n",
    "************\n",
    "We compared our new classifier to several old ones, including a Naïve Bayes classifier, a KNN classifier, and an ensemble classifier. In Naïve Bayes, the classification of instances is based on probabilities, and probabilities require a huge amount of computation. We were forced to implement this algorithm with significant attribute selection, since the program took more than an hour to run Naïve Bayes without it.\n",
    "Our KNN classifier turned out to be the second slowest, since it also requires tons of computation. The basic idea is this: for each instance to be classified, calculate the k nearest neighbors amongst the training set using the Euclidean distance algorithm, and then use a majority vote to pick the class label.\n",
    "For our ensemble, we chose to use a random decision forest. Our ensemble generates N trees, takes the M most accurate of those trees, then votes from amongst all of their predictions in order to choose the class label. We chose to use the TDIDT method to implement this algorithm. \n",
    "Our program has some surprising results regarding the accuracies of these classifiers. We both expected the support vector machine’s accuracy to be the highest, but that was not the case. Our support vector machine has the lowest accuracy of all the classifiers, coming it at a measly 60 – 70 percent. Naïve Bayes and KNN come next, and are nearly tied at about 82 percent accuracy, while the ensemble classifier has by far the best accuracy at around 90 percent.\n",
    "************"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "************\n",
    "Ultimately, our project consisted of cleaning the dataset, discretizing continuous attributes, creating a plot with SKLearn’s support vector machine, fitting the data to that plot for predictions, and finally, using stratified k-fold cross validation to compute the accuracy. The dataset proved to be challenging for us in several ways: the number of attributes, the presence of NA values, and lastly, the need to normalize the sale price. There was ultimately more work we had to do before implementing the classifier than there was for implementing the classifier itself. Our classifier’s performance wound up being less than satisfactory, and we definitely expected its accuracy to be higher. The low accuracy could either be a result of this particular dataset, or it could also be due to our lack of experience using support vector machines. We were unsure exactly how to choose the best C and gamma values for the SVC function, and we believe that if we had more time, and we figured out how to optimize these values, the support vector machine’s accuracy could improve drastically. But at the end of the day, we still learned a lot, and now we have a basic understanding of a new classifier to take with us into the real world.\n",
    "************"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
